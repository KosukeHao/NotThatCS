# 语料库

建立样本库是数据挖掘、人工智能研究不可缺少的基础工作。

语料库是指以文本类型为样本的数据集。有的语料库重在数量，有的则关注加工。

推荐资料：

图片分类的TED(李飞飞) https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures?language=zh-cn

儿童习得语言的资料TED记录：单词的诞生http://open.163.com/movie/2011/7/E/J/M77TUGO7R_M77TUSJEJ.html

## 语料库列表
- 语料库在线 http://www.cncorpus.org/
- 现代汉语语料库 http://ccl.pku.edu.cn/corpus.asp?item=1
- 古代汉语语料库 http://ccl.pku.edu.cn/corpus.asp?item=2
- 汉英双语语料库 http://ccl.pku.edu.cn/corpus.asp?item=3
- HSK动态作文语料库 http://202.112.195.192:8060/hsk/login.asp
- 北京口语语料查询系统 http://www.blcu.edu.cn/yys/6_beijing/6_beijing_chaxun.asp
- 现代汉语平衡语料库 http://rocling.iis.sinica.edu.tw/new/20corpus.htm
- LIVAC共時語料庫 http://www.livac.org/index.php
- 兰开斯特汉语语料库 http://ling.cass.cn/dangdai/LCMC/LCMC.htm
- 洛杉矶加州大学汉语语料库 http://www.lancs.ac.uk/fass/projects/corpus/UCLA/
- 中文新闻分类语料库 http://www.nlpir.org/?action-viewnews-itemid-145
- NLPIR 500万条twitter内容语料库 http://www.nlpir.org/?action-viewnews-itemid-263
- NLPIR微博博主语料库100万条 http://www.nlpir.org/?action-viewnews-itemid-232
- 現代漢語語料庫詞頻統計 http://elearning.ling.sinica.edu.tw/CWordfreq.html
- 欢迎关注新浪微博【对外汉语北京】
- 中文句結構樹資料庫 http://turing.iis.sinica.edu.tw/treesearch/
- 搜狗文本分类语料库 http://www.sogou.com/labs/dl/c.html
- 哈工大信息检索研究室对外共享语料库 http://ir.hit.edu.cn/demo/ltp/Sharing_Plan.htm
- 传媒大学文本语料库 http://ling.cuc.edu.cn/RawPub/
- 词语研究资源库 对外汉语北京 http://ling.cuc.edu.cn/newword/web/index.asp
- BFSU CQPweb多语言在线语料库检索平台 http://www.iresearch.ac.cn/paper/detail.php?ItemID=6358
- 英汉双语平行语料库 http://www.luweixmu.com/ec-corpus/
- babel 汉英平行语料库 http://icl.pku.edu.cn/icl_groups/parallel/default.htm
- 中国法律法规汉英平行语料库（大陆） http://corpus.zscas.edu.cn/lawcorpus1/index.asp
- 国家语言资源监测与研究中心 http://www.clr.org.cn/
- BCC语料库 http://bcc.blcu.edu.cn/
- 北外语料库语言学 http://www.bfsu-corpus.org/
- 美国当代英语语料库 http://corpus.byu.edu/coca/

## 样本收集方法

## 语料库加工方法

### 词语切分

#### 汉语自动分词技术

1. 概念
通过计算机把组成汉语文本的字串自动转换为词串的过程
2. 目的
    - TTS或语音合成： 只有正确切词，才能知道正确的发音
    - 信息检索： 分词有助于提高信息检索的准确率
    - 词语的计量分析：词频统计等
    - 深层汉语分析的基础：句法分析、语义分析等
3. 基本方法
    - 最大匹配法(MM)
        - 正向最大匹配法(MM)
        - 逆向最大匹配法(RMM)
4. 评价标准
    - 准确率(precision)
      准确率（P）＝切分结果中正确分词数/切分结果中所有分词数*100%
    - 召回率(recall)
      召回率（R）＝切分结果中正确分词数/标准答案中所有分词数*100%
    - F-评价(F-measure综合准确率和召回率的评价指标)
      F-指标＝2PR/(P+R)
5. 关键问题
    - 切分歧义（消解）：一个字串有不止一种切分结果
    - 未登录词识别：专有名词、新词

### 词性标注

所谓词性标注(Part of Speech tagging)就是根据句子上下文中的信息给句中的每个词一个正确的词性标记，即确定每个词的词性是名词、动词、形容词或者是其他词性。词性标注主要是针对多标记词 (即有多种词性的词)和未登录词(即在训练语料中未出现的词)进行的。

词性标注是自然语言处理领域的基础，可以提高信息检索的效果和效率，它在信息检索领域有着非常重要的作用。国内外该方面研究人员很重视它，成功设计出很多词性标注模型。归纳起来，比较典型的标注算法有： 

(1)基于规则的方法

基于规则的标注系统与系统设计者的语言能力有关，规则集直接体现了设计者的语言能力。不幸的是，要对某一种语言的各种语言现象都构造规则的话，将是一项很艰难也很耗时的任务。基于规则的标注系统另一个常见问题是：当根据规则判断一个词的词性时可能面临多种选择，如果不根据上下文则很难做出正确的选择。

(2)基于统计的方法 

20世纪80年代初，随着经验主义方法在计算语言学中的重新崛起，统计方法在词性标注中占据了领导地位。对于给定的输入词串，基于统计的方法先确定其所有可能的词性串，然后对它们分别打分，并选出得分最高的词性串作为最佳的输出。常见的方法有基于N元模型的方法和基于隐马尔科夫模型的方法。

参考：王丽杰, 车万翔, 刘挺. 基于SVMTool的中文词性标注[J]. 中文信息学报, 2009, 23(4):16-21.

### 命名主体识别

命名主体识别是指识别句子中特定类别的词语，包括但不限于：地名（Location）、人物（Person）、组织（Organization）、政治实体（Geo Political Entity，GPE）和杂项（MISC）。目前比较有效的命名主体识别的算法是基于条件随机场（CRF）的算法。

### 文本关键字提取算法

文本关键字提取是指对于一段文本，提取出一组关键字来表征这段文本。最常用也是最简单的一种提取算法是TF-IDF算法：

    TF-IDF算法全名Term Frequency-Inverse Document Frequency，是一种基于词频统计的算法，TF就代表词频，IDF可以视作一种权重。TF-IDF算法在中文自动摘要、信息检索、关键字提取等各种领域使用非常广泛。
    TF-IDF算法的步骤如下：
		第一步，计算TF。统计某个词在文章中的出现次数，然后除以文章的总词数，得到标准化的词频。如果只是文章内比较，可以不用除总词数。
			TF(s)=s在文中出现的总次数/文章的总词数 ,	
			或 TF(s)= s在文中出现的总次数	
		第二步，计算IDF。通常这是根据一个语料库预先就计算好的，计算公式有很多种，这里采用非平滑化的公式:
			IDF(s)=log⁡(语料库文档总数/(出现了s的文档数+1))	
		第三步，计算TF-IDF值，公式如下：
			TF-IDF(s)=TF(s)*IDF(s)	
		第四步，按照降序排序TF-IDF的值，取排在前面的前几个词。


## 其他相关内容
### 构建语料库的原则
语料库应该具有代表性、结构性、平衡性、规模需求并制定语料的元数据规范，各个原则具体介绍如下：

* 代表性：在应用领域中，不是根据量而划分是否是语料库，而是在一定的抽样框架范围内采集而来的，并且在特定的抽样框架内做到代表性和普遍性。
* 结构性：有目的的收集语料的集合，必须以电子形式存在，计算机可读的语料集合结构性体现在语料库中语料记录的代码，元数据项、数据类型、数据宽度、取值范围、完整性约束。
* 平衡性：主要体现在平缓因子：学科、年代、文体、地域、登载语料的媒体、使用者的年龄、性别、文化背景、阅历、预料用途（私信/广告等），根据实际情况选择其中一个或者几个重要的指标作为平衡因子，最常见的平衡因子有学科、年代、文体、地域等。
* 规模性：大规模的语料对语言研究特别是对自然语言研究处理很有用的，但是随着语料库的增大，垃圾语料越来越多，语料达到一定规模以后，语料库功能不能随之增长，语料库规模应根据实际情况而定。
* 元数据：元数据对于研究语料库有着重要的意义，我们可以通过元数据了解语料的时间、地域、作者、文本信息等；还可以构建不同的子语料库；除此外，还可以对不同的子语料对比；另外还可以记录语料知识版权、加工信息、管理信息等。

参考：[【NLP】大数据之行，始于足下：谈谈语料库知多少](http://www.cnblogs.com/baiboy/p/ylk.html)

### 双语语料库对齐

语料库可根据它所包含的语言种类的数目分为单语语料库和多语语料库，其中最典型的多语语料库是包含两种互译语言文本的双语语料库(Bilingual Corpora)。由于双语库含有两种不同语言之间的对照翻译信息，所以它在自然语言处理的许多研究和应用领域，如语言学习、机器翻译、双语词典和术语库的建立等方面，都具有相当高的研究和使用价值。
建立一个具有实际应用价值的双语语料库，最重要也是最关键的技术之一就是对齐，即从互译的不同语言文本中找出其互译片断的过程。由于文本的组成单位可以是篇章、段落、句子、短语、单词、字节，所以对齐的单位也分为不同级别。

根据应用目的不同，当前的对齐工作有以下两种输出结果：
(1) 表示成两种语言文本的划分形式，即将双语文本同时划分成相同多个细粒度的对齐单位，源文本的第n个单位与译文文本的第n个单位相对应。
(2) 表示成双语文本图(bitext map)，即将源文和译文中的匹配单位所在的位置分别看成平面坐标系中的横坐标和纵坐标，然后通过坐标(x, y)对应的值来表示源文中的x与译文中的y位置是否对应。

总的来说，双语库对齐的共同难点在于：
(1) 人工翻译的复杂性。由于对其所用到的语料大都来自人工翻译的结果，所以人工翻译的一些特点，造成了双语库对齐的许多困难。比如对于句子级的对齐，由于翻译中并非都是句到句的翻译，因此存在一对一的多种翻译模式，这种翻译模式的复杂性加大了对齐的难度。特别对于更细粒度（如单词）级别的对齐，这种现象更普遍。另外，人工翻译过程中有意或无意的增删、文章次序重组、自由式翻译以及个人的不同翻译喜好等都会影响对齐的困难程度。
(2) 不同语言之间的差异。由于对齐在不同语言之间进行，因此不同语言之间不同的语法规则、文化传统、表达方式、习惯用法甚至字符编码方式都会加大对齐的困难程度。
(3) 互为翻译的判断标准难以建立。对于人来说，多数情况下能够通过语义的理解作出正确的判断；而对于计算机，将这种语义的理解形式化在当前是不现实的，而制订一个现实的可计算的判断标准并不容易。
(4) 文本预处理工具的处理效果难以保证。对双语文本进行对齐时，必须首先对文本进行预处理，而处理效果的好坏直接影响到后续对齐的困难程度。
(5) 电子文本的噪声纷繁复杂。电子文本中可能存在大量表格、图形等格式信息，或各种拼写、语法错误，甚至无法区分段落、句子边界，这些都大大影响了文本间的对齐过程。

参考：《汉英双语语料库自动对齐研究》
http://www.cnki.net/KCMS/detail/detail.aspx?QueryID=0&CurRec=1&recid=&filename=2006191116.nh&dbname=CDFD9908&dbcode=CDFD&pr=&urlid=&yx=&uid=WEEvREcwSlJHSldRa1FhdkJkdjAzeWtZL3ZXNncvM2tUSHIyVnZQZEZNWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&v=MTk5MTJGQ2puVTc3TlYxMjdHTEt4SDlETnFaRWJQSVI4ZVgxTHV4WVM3RGgxVDNxVHJXTTFGckNVUkx5ZVorZHU=

### NLTK库及其包含的语料库介绍
	
TF-IDF(s)=TF(s)*IDF(s)	(3-23)
NLTK是斯坦福大学开发的处理自然语言的python库，主要包含如下功能：

* 语料库：提供了经典书籍，词典，演讲，网络语言，论坛等各种语料库；
* 断句与分词：可以方便地对文章，段落进行分词；
* 词频统计：计算句子或者文章中每个单词的频率；
* 同义词与词态：单词的同义词「WordNet」和单词词态「过去式，进行时等」的还原；
* 词性的标注：动词，名词，形容词和副词等的标注；
* 分类算法：例如常见的信息熵，朴素贝叶斯算法，最大信息熵模型等；

许多英文自然语言处理的研究都建立在NLTK提供的语料库上，其中比较常用有以下几种：

* `gutenberg`：古登堡语料库，古登堡计划致力于将文化作品的数字化和归档，古登堡语料库选择了 部分文本，包含了一百七十万字。
* `webtext`：网络文本语料库，网络和聊天文本
* `brown`：布朗语料库，按照文本分类好的500个不同来源的文本
* `reuters`：路透社语料库，1万多个新闻文档
* `inaugural`：就职演说语料库，55个总统的演说

安装`NLTK库`后利用`from nltk.corpus import <语料名>`即可导入相应的语料库。此外，还有一些仅是词或短语以及一些相关信息的集合，叫做词典资源。

* 词汇列表语料库：`nltk.corpus.words.words()`，所有英文单词，可以用来识别语法错误
* 停用词语料库：`nltk.corpus.stopwords.words()`，用来识别那些最频繁出现的信息量较小的词
* 发音词典：`nltk.corpus.cmudict.dict()`，用来输出每个英文单词的发音
* 比较词表：`nltk.corpus.swadesh()`，多种语言核心200多个词的对照，可以作为语言翻译的基础
* 同义词集：WordNet，面向语义的英语词典，由同义词集组成，并组织成一个网络

参考：
[NLTK笔记：简介与环境搭建](http://blog.ourren.com/2015/02/05/nltk_note_environment_install/)

[自己动手做聊天机器人 三-语料与词汇资源](http://www.shareditor.com/blogshow/?blogId=65)


### 维基百科中文语料库

维基百科的英文语料库因其高质量早已被广泛运用，其提供了中文版的语料库后，
逐渐在中文自然语言处理中广泛使用，其资源获取非常方便，可直接在[Wiki Dump](https://dumps.wikimedia.org/zhwiki/)上下载，其更新速度也很快，最新一次备份时间为2016年10月1日。虽然中文维基百科中的文字均是繁体，但是在经过简单的繁体到简体转换就能方便地使用，对该语料库文档的解析有非常多的成熟工具，直接使用开源工具即可完成正文的提取。

对于该语料库的使用方式和应用场景也在多篇博文中提到：

* [中英文维基百科语料上的Word2Vec实验](http://www.52nlp.cn/中英文维基百科语料上的word2vec实验)
* [维基百科简体中文语料的获取](http://licstar.net/archives/262)
* [用wiki百科中文语料训练word2vec模型](http://blog.csdn.net/hereiskxm/article/details/49664845)

参考：
[自然语言处理之语料库资源](http://blog.just4fun.site/NLP-corpus.html)

### 搜狗实验室数据资源 （http://www.sogou.com/labs/）  
	1. 包含了5大类13种数据资源  
		1.1 评测集合   
			1.1.1 搜索结果评价   
				简介：判断搜索结果与查询的相关性，是否符合搜索意图。  
				数据：完整版4326条，数据格式“查询词\t相关的URL\t查询类别”。  
				相关任务：基于互联网语料的信息检索。  
				相关技术：2.1。   
			1.1.2 话题跟踪及检测评价   
				简介：评测新闻话题跟踪及检测效果。  
				数据：完整版953条，数据格式“URL\t话题名称”。  
				相关任务：文本分类。  
				相关技术：2.2  
			1.1.3 文本分类评价   
				简介：评估文本分类结果的正确性。    
				数据：94条，数据格式“URL前缀\t对应类别标记”。  
				相关任务：文本分类。  
		1.2 语料数据   
			1.2.1 互联网语料库  
				简介：来自互联网各种类型的1.3亿个原始网页。  
				数据：完整版1TB，迷你版10个页面数据。  
				相关任务：相关性排序，文本分类，新词发现，机器翻译，分词。  
				相关技术：2.3，2.4      
			1.2.2 链接关系库  
				简介：包括对应互联网语料库内文档的链接关系列表。  
				数据：完整版90GB，迷你版1000条URL对照表和1000条链接关系。  
				相关任务：相关行排序，链接分析，反垃圾。  
				相关技术：2.4  
			1.2.3 SogouRank库  
				简介：互联网语料库中各页面的重要程度评级。  
				数据：完整版90GB，迷你版1001条，数据格式“URL\tSougouRank”。  
				相关任务：相关性排序。  
			1.2.4 用户查询日志  
				简介：搜索引擎部分网页查询需求及用户点击情况的网页查询日志数据。  
				数据：完整版1.9GB，迷你版10000条，数据格式“访问时间\t用户ID\t[查询词]\t该URL在返回结果中的排名\t用户点击的顺序号\t用户点击的URL”。  
				相关任务：相关行排序，用户兴趣挖掘，查询扩展，新词发现。  
		1.3 新闻数据  
			1.3.1 全网新闻数据  
				简介：来自5个新闻站点共83个频道的新闻数据，提供URL和正文信息。  
				数据：完整版1.02GB，迷你版200条新闻数据。  
				相关任务：文本分类，事件检测跟踪，新词发现，命名实体识别，自动摘要。  
				相关技术：2.2  
			1.3.2 搜狐新闻数据  
				简介：来自搜狐新闻共18个频道的新闻数据，提供URL和正文信息。  
				数据：完整版65GB，迷你版200条新闻数据，论文（见2.2）版953条。 
				相关任务：文本分类，事件检测跟踪，新词发现，命名实体识别，自动摘要。  
				相关技术：2.2  
		1.4 图片数据  
			1.4.1 互联网图片库  
				简介：来自sougou图片搜索索引的280多万张抓取图片及标注数据集合。    
				数据：完整版269GB。  
				相关任务：基于文本/内容的图片检索。  
			1.4.2 互联网图片库2.0  
				简介：1000万张互联网图片和相关文本信息，以及识图搜索结果的人工标注集合。    
				数据：完整版635GB。  
				相关任务：基于内容的图片检索。  
		1.5 自然语言处理相关数据   
			1.5.1 互联网词库  
				简介：基于互联网语料环境的高频词对应的词频、词性信息。      
				数据：完整版1.3MB，共157202个词。  
				相关任务：中文词性标注，词频分析。  
			1.5.2 中文词语搭配库  
				简介：基于互联网语料的字词搭配关系统计。        
				数据：完整版共18399496组，迷你版共100000组。  
				相关任务：中文输入法，文字到语音转化，语音识别。  
				相关技术：2.5，2.6  
	2. 提出了一些相关技术并进行了实验  
		2.1 基于点击数据分析，自动对搜索结果进行评估。(http://www.sogou.com/labs/paper/Automatic_Search_Engine_Performance_Evaluation_with_Click-through_Data_Analysis.pdf)   
			- 将搜索分成三种情况：找特定网址、找信息、处理事务。“找特定网址”具有明确的目标特征。因此这里仅考虑这类搜索情况。  
			- 通过统计搜索词q与搜索结果中点击量最多的链接r，统计（q，r），并在下次给出搜索结果时让r更靠前，如此不断验证。  
			- 从2006年6月至2007年1月，对sougou.com的检索和点击数据进行统计。  
			- 准确率为97%左右。错误结果中，一些网址是正确网址的子网站。例如，搜索163通常会定位到163邮箱mail.163.com，而非www.163.com。        
		2.2 网络环境下自动进行在线新闻事件的生成。(http://www.sogou.com/labs/paper/Automatic_Online_News_Issue_Construction_in_Web_Environment.pdf)    
			- 方法分三步：  
				1）话题检测，将新出现的文本内容聚类成候选话题。  
				2）话题对比，候选话题与既有的话题比较，并入既有话题或称为新的话题。  
				3）根据相关话题生成新闻事件。  
			- 具体做法：   
				1）预处理：提取页面内容，分句，除去无用句，对单词进行标记（中文进行词汇划分），词性标注，识别命名实体，删除“应删除词”（例如“的”），最终每篇文章生成一个词向量。  
				2）计算t时刻词w的“词频”，每篇文章表示表示为t时刻的一个n维向量，使用增量TF-IWF模型计算各维度权重weight t(d,w)并归一化处理。  
				3）使用余弦相似度算法计算两篇文章间的相似度。  
				4）使用算术平均加权配对组（UPGMA）的聚类方法，将新文章聚类至候选话题中。    
				5）对候选话题与既有话题相比较，并入并更新既有话题，或成为新的话题。  
				6）类似地将各话题聚类到新闻事件中，并不断更新新闻事件或生成新的新闻事件。    
				7）自动对各新闻事件加入网络上的相关博文、评论和图片、音频、视频等。 
			- 实验数据：  
				数据集1：含350个新闻页面，87个话题。2007年3月到4月，均关于搜索引擎公司。中文新闻网站，包括新闻报道和新闻回顾。最多的话题包含20个文章，最少1个。  
				数据集2：含953个新闻页面，108个话题。2007年4月22日的搜狐体育新闻频道。最大话题有151个文章，最少1个。  
				数据集3：含24872个新闻页面，选自多个中文新闻网站和1339篇博文和评论文章。  
			- 实验结果：  
				1）聚类方法的选择：direct-I2-IDF、direct-I2-IWF、rbr-H2-IDF、rbr-H2-IWF、graph-jacc-IDF、graph-jacc-IWF、agglo-upgma-IDF、agglo-upgma-IWF这八个算法中，“aggloupgma-IWF”算法更优。  
				2）IWF与IDF比较：IWF模型的效果更平滑更优。  
				3）冗余句子的去除（RSR）：新闻中冗余句子较少，RSR有效果但不明显。  
				4）标题的使用及权重：加标题表现更好；仅短文加标题比全部加标题更好；标题权重比正文相对更大表现更好。    
		2.3 使用与查询无关的特征对网络信息检索数据进行清洗。(http://www.sogou.com/labs/paper/Data_Cleansing_for_Web_Information_Retrieval_using_Query_Independent_Features.pdf)  
			- 若使用普通的链接分析方法，基于网页的被点击概率，而非页面的有用度。因此使用与查询无关的特征。  
			- 结合利用了链接分析和页面布局分析，进行全局规模的数据清洗。  
			- 使用朴素贝叶斯学习算法，在低维度实例空间上高效实用，且不需要原有数据集的先验知识。  
			- 将5个特征：文字长度，链接文本长度，搜索排名值，导入链接数量和导出链接数量综合应用，比仅使用单独一个特征效果更好。  
			- 清洗后选出的高质量页面占全部的52%（.GOV数据集）和5%（sogou数据集），召回率>90%。即牺牲了10%的正确搜索结果，大大节省了存储空间。  
			- 同时能够消除30%的垃圾页面和15%的低质量页面。  
		2.4 一种基于链接分析的垃圾页面检测算法。(http://www.sogou.com/labs/paper/R-SpamRank_A_Spam_Detection_Algorithm_Based_on_Link_Analysis.pdf)  
			- 一些人试图误导搜索引擎以提升网页的搜索排名，方法主要是基于内容和基于链接两种。这里提出一种半自动的检测基于链接的垃圾网页的方法。  
			- 使用人工识别的垃圾页面黑名单作为种子，设置高RSR值。根据链接到本页面的情况，反向传播RSR值，并不断迭代直到各页面值稳定。  
			- 测试数据为sogou.com的500万个网页，迭代50次。人工分析后，不能打开的页面赋值0，好的页面赋值1，半垃圾页面赋值2，纯垃圾页面赋值3。  
			- 将结果中RSR值最高的1万个页面中的前100个和最后100个取出做人工检查。发现99%是垃圾页面。证明算法有效。  
			- 发现垃圾页面集中在两个域名，说明算法能够检测链接工厂。  
			- 做域名清理，即删除3个链接工厂下的所有页面后再分析。剩下的178个页面中仍有87.1%的垃圾页面。  
			- 前5大链接工厂产生了99.1%的垃圾页面。  
		2.5 基于相对条件熵的搭配抽取方法。(http://www.sogou.com/labs/paper/Wangdaliang_JoBUPT_07.pdf)   
			- 在自然语言处理中，研究搭配组词项之间的内在倾向性。提出使用相对条件熵比传统的使用互信息评价二元相关性更优。  
			- 词语在语料库中出现越频繁，越容易失去倾向的特性。  
			- 绝大多数次的左搭配力大于自己的右搭配力。  
			- 左搭配倾向强的多是定语；右搭配倾向抢的多是宾语。  
			- 自然文本的搭配抽取方法：  
				1）预处理：提取文本，分词，词性标注，停用词过滤，词频过滤。    
				2）数据统计：利用词性过滤模板、滑动窗口生成搭配候选二元组统计矩阵，同时统计二元同现次数、构成次的词频以及样本总词频。  
				3）搭配抽取：计算候选二元组左、右搭配倾向强度以及搭配整合强度，依据阈值输出搭配抽取结果。  
			- 对sogou.com的1亿多个中文页面语料库数据进行实验。预处理得搭配候选二元组35万个，自动获取搭配词对3.5万个。人工验证得效果比互信息法好。  
		2.6 多策略融合的搭配抽取方法。(http://www.sogou.com/labs/paper/Wangdaliang_JoTHU_08.pdf)  
			- 搭配抽取中，需要识别频繁二元组和稀疏二元组，而排除无关二元组。  
			- 互信息法可作为二元组无关性的度量方法，用于排除大部分无关二元组。  
			- 卡方检验法比t检验法更适合于刻画二元组的相关性，且能很好识别频繁二元组，但对稀疏二元组不行。  
			- 对数似然比检验法可用于识别稀疏二元组。  
			- 对sogou.com的1亿多个中文页面语料库数据进行实验。预处理得搭配候选二元组35万个，自动获取搭配词对5万个。人工验证得多策略融合法效果更好。  
			
			
### HSK动态作文语料库
http://202.112.195.192:8060/hsk/login.asp

数据来源：母语非汉语的外国人参加高等汉语水平考试（ HSK 高等）作文考试的答卷语料库。语料库 1.0 版收入语料 10740 篇，约 400 万字，于 2006 年 12 月下旬上网试运行。经修改补充，语料库 1.1 版语料总数达到 11569 篇，共计 424 万字。共有两种版本：标注语料和原始语料。标注语料是把考生作文答卷人工录入电脑并经人工标注各种中介语偏误的语料，原始语料指的是考生原始作文的电子扫描语料。

相关技术：
一、语言计算
1.自然语块研究
自然语块提出以应对中文词边界模糊复杂的困难。以自然标注信息分割连续文本，以语块粒度进行建模。语块分析能够将复杂语句划分为若干较细粒度的片段，从而有效降低信息处理复杂度。某些研究中自然语块特指在海量语料中稳定、频繁出现，具有明显边界特性的语言片段，其不受语法规则约束，在处理汉语边界划分问题上具有其优势；且划分具有柔性，不同应用不同划分。研究自然语块合理性评估方法，并通过分析自然语块对海量词典的覆盖度，考察自然标注信息对汉语词汇知识的描述能力；从同构性角度分析自然语块与中文分词、汉语韵律短语的一致程度，对自然语块分析性能作出评价。
2.术语研究
（一）术语资源建设
目前整理通用领域科技术语超过200万条目，整理加工科技文献语料库超过1亿字。全国自然科学名词审定委员会在多个领域组织了科技名词审定工作，在术语审定、术语规范化、术语使用领域有丰富经验。
（二）术语网及术语图谱
“术语信息网”网站http://term.blcu.edu.cn，提供科技术语的检索和术语自动翻译。在术语检索部分，提供术语译文、相关术语、术语例句、英文术语等信息的检索。

二、字形计算
基于汉字书写的现状，以手写汉字为研究对象，采用数字媒体处理、模式识别、机器学习等技术，对汉字书写正确性和规范性的自动评测进行理论及技术上的研究，实现无人值守的汉字书写教学和评测自动化系统。
1.总结符合规范的汉字字形结构的本质特征以提供足够的样本数据；2.改进传统汉字书写教学方法并为考试的信息化提供技术。

2.汉字书写评测技术的研究与传统的OCR（即：光学字符识别）技术具有本质区别。汉字书写评测技术的根本任务是在已知目标汉字的前提下，评价手写样本的规范性；OCR技术的目标是在未知目标汉字的前提下，找到与手写样本最相似的模板汉字。虽然它们都涉及汉字字形的特征匹配，但是前者所要求的匹配结果更加准确，匹配程度更加精细，这就为字形匹配技术提出了更高的要求。所以，虽然OCR技术已经得到长足发展，并对汉字书写评测具有一定的借鉴意义，但是由于研究任务的不同，还需要深入发掘汉字书写评测的技术内涵。

研究近况：
研究包括三类：
第一类研究主要集中在汉字演化、组词、发音等方面的训练，这些成果有助于汉字的识别和自主学习，但是不涉及汉字的书写评价。
第二类研究表现为简单的计算机辅助汉字书写练习，这些成果一般借助键盘、鼠标等输入设备，通过临摹、描红或多媒体手段，让书写者完成汉字书写过程。这是改进汉字教学的一种有效方法，但很少涉及汉字书写正误和规范的考核和评测。
第三类研究探索了对汉字书写的笔顺、笔画、笔向等书写规则的评测。但这些研究还停留在实验阶段，它们能够评测的汉字集合较小，评价的侧面还只是表现为书写规则的评价，还不能对汉字书写的规范性给出定量的评价。

未来发展目标：
以评测的全面化、精确化、智能化为主要目标，探索更合理的汉字形式化描述方法和人机交互方法。

目前研究遇到的问题：
离线手写汉字的笔画还原技术；在线手写汉字的字形匹配技术；面向书写评测的汉字形式化描述方法；汉字书写评判侧面的挖掘方法；汉字书写评测的可视化反馈技术等。


### NLP开源工具包
中文词法分析
THULAC：一个高效的中文词法分析工具包
http://thulac.thunlp.org/
包括中文分词、词性标注功能。已经提供C++、Java、Python版本。

中文文本分类
THUCTC: 一个高效的中文文本分类工具
http://thuctc.thunlp.org/
提供高效的中文文本特征提取、分类训练和测试功能。

THUTag: 关键词抽取与社会标签推荐工具包
https://github.com/YeDeming/THUTag/
提供关键词抽取、社会标签推荐功能，包括TextRank、ExpandRank、Topical PageRank（TPR）、Tag-LDA、Word Trigger Model、Word Alignment Model等算法。

PLDA / PLDA+: 一个高效的LDA分布式学习工具包
https://code.google.com/archive/p/plda/

知识表示学习
知识表示学习工具包
https://github.com/mrlyk423/relation_extraction
包括TransE、TransH、TransR、PTransE等算法。

考虑实体描述的知识表示学习算法
https://github.com/xrb92/DKRL

词表示学习
跨语言词表示学习算法
http://nlp.csai.tsinghua.edu.cn/~lzy/src/acl2015_bilingual.html

主题增强的词表示学习算法
https://github.com/largelymfs/topical_word_embeddings

可解释的词表示学习算法
https://github.com/SkTim/OIWE

考虑字的词表示学习算法
https://github.com/Leonard-Xu/CWE

网络表示学习
文本增强的网络表示学习算法
https://github.com/albertyang33/TADW

Natural Language Toolkit
http://www.nltk.org/

哈工大LTP：http://ir.hit.edu.cn/

### 语料库数据化开发
虽然数据库目的性和实用性更强，但需要大量人力物力的投入，发展比较缓慢。针对这种情况，西方语言学者就开始直接对语料库进行深加工处理，把语料库的建设从追求量的扩充向语料数据化方向转化，即利用标注、数据挖掘技术和计算机自动运算的方法对无序的语料索引行进行梳理、统计、分类、归纳，按用户需求有针对性地呈现各种典型的词汇数据。美国当代英语语料库( Corpus of Contemporary American English，以下简称 COCA) 就是一个典型的代表。

COCA由美国杨百翰大学( Brigham Young University) 的 Mark Davies教授主持开发，语料规模达4.5亿词，是美国目前最新的当代英语平衡语料库。自 2008 年 2 月 20 日在互联网上正式推出以来，每年都要至少做两次语料更新。语料库的数据化主要体现在将杂乱无序的语料变得有条有序，能根据用户的需要提供相对准确的词汇语言数据，其关键就是“智能化”索引，而索引的基础是对语料的标注和基础数据库的支持。下面就从这几个方面谈谈COCA数据化的功能特征。 

1 语料库的标注 

词典编纂者最想从语料库中获取的信息大多是语词的各种语言属性，包括形态、词类、句法模式、搭配成分、语义表征和使用语境等，因此需要对库内的各种用词进行标注。COCA 在 SWECCL 词类赋码的基础上设计了150多种标签，对全部4.5亿语料进行了逐条标注。譬如，连词类7个，如 CC(并列连词) 、CS(从属连词) 、CCB(转折并列连词)等; 形容词4个，如JJ(普通形容词)、JJR(形容词一般比较级)、JJT(形容词一般最高级)、JK(连接形容词); 限定词类13个，如 DA(前置或后置限定词) 、DA1( 后限定词单词形式) 、DAR(后限定词比较形式) 等; 名词类22个，NN(普通名词)、ND1(方向性单数名词)、NN1(单数普通名词)、NN2(复数普通名词) 、NNL1(单数方位名词) 等; NNO(数量词)、NNT1(单数时间名词)、NNU(度量单位词) 等; 动词31个，如 VB0(动词原形)、VDD(动词过去式)、VBM(系动词)、VBG(现在分词) 、VBN(过去分词)等; 代词19个，如PN(不定代词) 、PNQO(宾格 WH 代词) 、PNQS(主格 WH代词)和PNX1(反身不定代词) 等。其他还有副词、介词、冠词、程式(FO) 、未分类词( FU) 和外来词( FW) 等。 

除词类外，COCA 还对所有语料做了词汇语域和时间分布的标注，因为语词的意义和用法 与语域以及时间有着密切关系。语域维度分为口语、小说、流行杂志、报纸和学术期刊五大类型，语料按这五个类型基本呈均匀平衡分布; 时间维度分为: 1990—1994、1995—1999、 2000—2004、2005—2009、2010—2012 等五个时段，用户可以从这两个维度查询任何一个词的分布频率。为了语词的形态变体和同义词查询，库内还配置了词的屈折变化和同义词数据库。 

2 语料库的索引 

COCA 根据标注和用户的查询需要设计出一套索引句法( Search Syntax) 来满足“智能” 检索的需要。主要分以下几类: [pos](精确词类［vvg］) 、[pos* ]( 各种词类[v* ])、[lemma](原型词形态变体[speak])、[=word](同义词) 、word|word(两词比较)、*xx(以某前缀开头的词) 、x? xx( 含某字母的词) 、x? xx* ( 含某字母 + 某词缀的词) 、-word( 某词前成 分，若要限定词类则可加词类标签，如-[nn* ]) 等。利用上述句法成分和词类标签可以组合成各种复杂的索引句法结构，以便词典编纂者准确查找所需的各种信息。 

3 语料的智能化检索和显示 

这里说的“智能”并不是说它具有抽象思维和随机应变的能力，而是指检索系统通过识别语料库中的标注代码，按特定的索引句法提取语料数据。这样，用户便可以根据自己的查询需要，按一定句法规则来组织“检索模式”，语料库便可以比较准确地调出他们所需的各类信息。值得一提的是，即使是无限制的普通检索，COCA 也能做不同的结构和句法/搭配成分分类显示，如相同结构的例句放在一起，各种句法成分用不同颜色显示: 名词为蓝色、动词为紫色、形容词为绿色、副词为棕色、代词为灰色 、介词为黄色。这样，可以使用户对所显示内容一目了然。

参考：《语料库数据化发展趋势及词典学意义——兼谈美国当代英语语料库的数据化特征》
http://www.cnki.net/KCMS/detail/detail.aspx?QueryID=14&CurRec=25&recid=&filename=CSYA201505001&dbname=CJFDLAST2015&dbcode=CJFQ&pr=&urlid=&yx=&uid=WEEvREcwSlJHSldRa1FhcTdWYVFpK1NSeUY1MUgweVkrdjJxQ1ZqenJCQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&v=MDA2MTBUTXFvOUZaWVI4ZVgxTHV4WVM3RGgxVDNxVHJXTTFGckNVUkx5ZVorWnBGeWptVjczSkpqN1NiN0c0SDk=


### 关于语料库开发的工具介绍:

1 索引工具

索引（Concordance）是呈现检索项和其共现语境的一种方式。检索项由词或短语以及词性赋码标记、通配符（wildcard）和正则表达式（regular expression）等构成。索引行可以按各种条件排序，常见的方法是按中心词左边或右边第N个词的字母顺序排序。研究者可对索引行数据做进一步分析，如搭配、类连接等，发现语言中反复出现的现象，从而揭示人们使用语言的模式（pattern）。

BFSU Collocator：搭配分析工具。搭配词是指在一定跨距内出现在节点词周围的词语，为了更好地研究节点词和搭配词间的意义关系，研究者可以利用各种统计算法计算两者间的搭配强度以排除搭配词中出现的高频功能词如“the”、“of”等。英语学习者可以通过搭配了解单词的典型用法，加深对词汇意义的理解。 BFSU Collocator提供MI、MI3、Z-Score、T-Score、Log-log和Log-likelihood等六种统计算法计算搭配强度，研究者可以根据具体研究问题选择合适的算法进行搭配研究。

BFSU Colligator：类连接分析工具。类连接是语法层面的搭配，包括语法类别之间以及词和语法类别之间的共现关系。研究者可以通过BFSU Colligator编写正则表达式，研究词类之间以及节点词与其周围词类间的关系。具体用法可参看许家金、熊文新（2009）。

BFSU CQPweb：基于网络的第四代语料库分析工具，能生成语料库词表、进行索引行分析、搭配计算和主题词分析。BFSU CQPweb采用了索引技术（indexing）提高了检索结果的响应时间，并可按照语料库中的各种元信息呈现检索结果。该平台目前已加载了多种语料库供用户免费使用，使研究者通过浏览器就可进行基于语料库的各种研究，降低了语料库语言学研究的技术门槛。BFSU CQPweb的网址为http://111.200.194.212/cqp/ ，具体用法可参看许家金、吴良平（待刊）。

BFSU CQPweb在线检索平台 网址http://111.200.194.212/cqp/

BFSU PatCount：BFSU PatCount用于统计文本中各种语言特征的出现频率，全面支持正则表达式及批量检索。研究者有时需要同时从文本中提取多种语言特征，比如Biber（1998）的多因素分析需要提取文本中的67种语言特征进行因子分析，如果逐一检索过于繁琐，而PatCount可以将67个检索式写入文本文件中，一次就可完成任务。输出结果以矩阵形式排列，每行为语言特征，每列为文本名称，便于后续进行因子分析。具体用法可参看梁茂成、熊文新（2008）。

ConcSampler & Concordance Randomizer ：在语料库研究中，研究者经常要面对成千上万条索引行，Sinclair（1999）建议每次随机抽取30条记录进行观察，总结其中的规律，然后再抽取30条记录，以此类推，直到无法观察到新的模式为止。BFSU开发的这两款工具可以帮助研究者或教师完成随机提取索引行的任务。
   
2 标注工具

标注是给语料库增添信息的过程，McEnery & Hardie（2012：29-31）认为可以把这些信息分为三类： 
  （1）元信息（Metadata）。如书面语中的文本类型（新闻、小说等）；口语中说活人的特征（性别、年龄等）。 
  （2）文本信息（Textual Mark-up）。如书面语中的段落、句子；口语中的停顿、重复等。 

（3）语言学信息（Linguistic Annotation）。如单词的词性、句子的结构和功能等。 
研究者根据上述三种标注类型可以从语料库中提取相应的信息开展研究，如女性和男性在口语中使用形容词的异同。

Metadata Encoder：元信息标注工具。可以在书面语语料库中添加文本作者、作者国籍和出版年代等信息；在口语语料库中添加说活人性别、年龄和社会地位等信息。用户可以在Metadata Encoder的配置文件中自行添加元信息的类别如<sex>、<age>等，然后在软件中针对每个类别进行标注，添加的信息会以下列格式保存在文本文件中：
   <sex>female</sex>
　　<age>22</age>
研究者可以根据文本中的元信息从语料库中提取某一类型的文本（如年龄在18-25之间的女性话语），建立子语料库（参看下面 Sub-corpus Creator的介绍）开展研究。

TreeTagger for Windows 2.0：TreeTagger是德国斯图加特大学Helmut Schmid开发的一款自动词性标注软件，采用宾州树库符码集（http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/Penn-Treebank-Tagset.pdf）。TreeTagger for Windows 2.0是其图形界面，支持英语、德语、法语、意大利语等四种语言的词性标注，同时还支持词形还原（lemmatization）功能。下面是该软件对句子“The cuisine of Xinjiang reflects the region's many ethnic groups and refers particularly to Uyghur cuisine.”的标注结果（用户可与下文BFSU Stanford POS Tagger的标注结果相比较发现两者符码的异同）：The_DT cuisine_NN of_IN Xinjiang_NP reflects_VVZ the_DT region_NN 's_POS many_JJ ethnic_JJ groups_NNS and_CC refers_VVZ particularly_RB to_TO Uyghur_NP cuisine_NN ._SENT
BFSU Stanford POS Tagger 1.0：Stanford POS Tagger是斯坦福大学自然语言处理小组开发的一款词性自动标注软件，采用宾州树库符码集（Marcus等：1993），符码准确率可达到96.97%（http://nlp.stanford.edu/software/pos-tagger-faq.shtml）。BFSU Stanford POS Tagger是其图形界面，它降低了原软件的操作难度，用户无需在命令行中输入命令和参数就可对文本进行符码。运行该软件前需安装JAVA虚拟机（http://www.java.com/en/download/）。下面是该软件对句子“The cuisine of Xinjiang reflects the region's many ethnic groups and refers particularly to Uyghur cuisine.”的标注结果（用户可与上文TreeTagger for Windows的标注结果相比较发现两者符码的异同）：The_DT cuisine_NN of_IN Xinjiang_NNP reflects_VBZ the_DT region_NN 's_POS many_JJ ethnic_JJ groups_NNS and_CC refers_VBZ particularly_RB to_TO Uyghur_NNP cuisine_NN ._.

BFSU Stanford Parser 1.0：Stanford Parser是斯坦福大学自然语言处理小组开发的一款句法自动切分软件，可以切分句子的短语结构（Phrase Structure）和依存关系（Dependency Relation）。BFSU Stanford Parser 1.0是其图形界面，它降低了原软件的操作难度，用户无需在命令行中输入命令和参数就可对文本进行句法切分。利用该软件对句子“The cuisine of Xinjiang reflects the region's many ethnic groups and refers particularly to Uyghur cuisine.”进行句法切分后可以输出以下两种格式：
   (1）括号格式的短语结构。其中NP、VP和PP分别代表名称短语、动词短语和介词短语
   (2）依存关系。其中det表示限定词与名词之间的关系；nsubj表示句子主语与谓语间的关系。其它依存关系可参考de Marneffe & Manning（2008）

BFSU Qualitative Coder 1.0 & BFSU Qualitative Explorer：在语法、语义和语用等层面的许多语言现象无法通过软件自动标注，如对话中言语行为（dialogue act）的种类（请求、疑问赞同等）、小说中人物话语和思想的呈现方式（直接应用、间接应用等）以及与某一语言特征如情态动词共现的其它语言特征，如主语的生命度（有灵、无灵主语），后续实义动词的语义种类（状态、过程动词等）、所在句子的类型（陈述句、疑问句等），都需要手工进行标注。研究者可以利用
BFSU Qualitative Coder自行设定语言现象的种类，在文本中进行手工标注。标注完成后可以用BFSU Qualitative Explorer统计各种语言现象出现的频率。
   
3 文本处理工具
BFSU Sentence Segmenter 1.0：我们通常以逗号、句号、问号和感叹号等标点符号作为英文句子间的分界点，但如果采取上述方法进行自动分句的话会将一些缩略词的词尾（如Dr.、Mrs.、Ph.等）误判为句子结尾，造成切分错误。BFSU Sentence Segmenter支持用户自定义缩略词表，可以通过扩充缩略词表提高句子切分的准确率。

Sub-corpus Creator：该软件利用语料库中的文本名称以及文本中的元信息建立研究者所需的子语料库。如CROWN语料库中以A、B、C开头的文件代表新闻语体，研究者可以用(A|B|C)\S+\.txt这一正则表达式选定这类文本。同时该软件还可以选择文本内容，如研究介绍中国的新闻报道可以在文本内容中输入表达式china|chinese。
##
Readability Analyzer 1.0：通过对文本平均词长、平均句长、形符/类符比等特征的统计计算英文文本的易读性。
   
4 数据驱动学习工具

数据驱动学习（Data-driven Learning）是指老师和学生通过检索语料库、分析索引行的方法教授/学习二语/外语的过程。这一方法强调学生的主动性，鼓励他们通过“发现式”的学习方法归纳语言在真实语境下的使用规律。

BFSU Sentence Collector 1.0：用于英语教学的索引工具，内置大学英语教材语料库（http://www.corpus4u.org/forum/showthread.php?t=3217）与四级词表。与上文介绍的索引工具不同的是其呈现方式为含有检索词的整句；另外用户可以根据句子长度和句子中的新词数（未出现在四级词表中的单词）来筛选例句。该工具支持正则表达式检索，如输入as \S+ as可以检索出含有as well as、as much as等短语的例句。

BFSU NewWord Marker 1.0：新词标记工具。根据用户指定的基准词表（如四、六级词表）输出特定文本中含有新词的句子，每句后列出未出现于词表中的新词以及该句所在的文件名称。进行新词标记前用户可以用BFSU Sentence Segmenter 1.0先对文本进行分句


###大数据语音语料库的社会标注

近年来，随着互联网和移动终端的普及，各种形式的大数据如洪水般涌来，为大数据语音语料库的过滤、标注和运用带来了挑战。而在技术研究上，将深度学习成功引入到声学模型训练中，使得语音识别研究获得重大突破］。相比传统 Gauss混合模型（ Gaussian mixture model，GMM）， 基于深度学习的深度神经网络（deep neutral network, DNN）模型显示出了对大数据训练语料的更强烈的需求。本文作者在实验中发现，每次增加训练语料规模，都会带来DNN 识别性能的明显提升，其性能的极限还远没有达到，而且语料数据本身的质量和丰富程度对性能影响也非常明显， 这说明大数据语料的积累和精细标注将会对DNN的更深入探索具有重要意义。
由于数据量巨大，在实际应用中不得不考虑标注的成本、效率和质量，这3个要素缺一不可。社会标注是指由大众用户产生的、对资源内容进行自下而上标注分类的体系，是利用大众智慧来对资源进行配置和挖掘的一种手段。社会标注是 Web 2.0基础上产生的，其关键技术在于使大众可以开放自由地分享内容, 而社会化的标签已经成为互联网上的一种重要的信息组织方式。
    
最典型的系统包括美国宾夕法尼亚大学早在2005年就开发出来的PennTags系统，以及美国密歇根大学在2008年推出的MTagger系统。社会标注的快速发展则是以提供标注服务的网站的流行为标志的，这些网站允许用户使用简短的字词对自己喜爱和关注的网页进行标注并保存在自己的账号中，极大方便了用户对个人网络资源的组织和管理。比较有名的网站有：网络书签系统Delicious，图片标注分享系统Flickr，对学术论文进行标注的文献检索系统 CiteULike ，对MP3音乐进行标注和偏好推荐的系统LastFM。而一些博客和论坛等，也或多或少提供了类似主题词的标签功能。

标注模型
    
社会标签是一种元数据， 由〈User，Resource，Tag〉三元组组成，并通过分析用户、资源和标签之间的关系进行标签理论和标签推荐的研究。然而，此类研究一般是仅分析对象间的关系，没有考虑资源本身的特征，标签的内涵也仅限于主题词。因此，针对语音数据的标注，将语音资源的特征考虑在内，并将标签对象拓展为更广义的标记，一种标记是与语音内容直接相关的词语串，另一种标记是与语音内容间接相关并具有一定标引功能的标签。
标注模型包含３类对象：参与标注的用户、 标记本身以及被标记的数据。本文的社会标注系统是一个六元组A={用户，标记，数据，节点之间表示标注关系的超编集，不包含重复词汇的词表，语音}。
    
在标注系统中，每个用户的标注是用大量标记体现出来的， 而每个数据将会获得多个用户的标记，每个标记都是用户针对数据的一种转录和理解， 这些标记体现了大众智慧。

该标注系统可以分成数据流控制和预处理模块、社会标注模块、质量自动控制模块、人工审核模块、 标注信息存储模块、标注信息运用模块６个部分。其中， 数据流从口语翻译系统源源不断流进， 数据均为终端用户的实际测试语音。由于用户和数据的响应并发量巨大，而标注系统的运行必须要与数据流相匹配。因此在数据送入标注模块之前增加数据流缓存环节，并根据标注进度来进行有效调控。同时，由于原始数据流中混有大量无效的语音样本，这些语音是没有必要进行标注的，包括太短的语音、没有发声的语音、发音内容杂乱无章的语音等， 通过信号层检测和ASR识别结果的置信度进行过滤。

标注任务的组织和分解

社会标注的难点在于如何保证系统能够高效运行，如何让大众用户持续为系统作出贡献。“兴趣＋收获 ＋ 报酬”的三位一体的社会标注方案，通过对标注任务的有效组织和分解来增强趣味性，通过将标注任务与语言学习进行结合来提高用户的收获感和成就感，通过对用户标注数量和质量的积分累计来进行报酬兑换， 以便进一步提高用户标注的积极性。

标注质量控制机制

社会标注在效率和成本方面具有优势， 而质量控制则是其短板。因此，首先必须制定相应的标注规范，例如词汇的单复数、连接符号、标记符号、口语现象处理、异常处理等，这些规范会在用户注册时进行展示，用户同意条款才能开始标注。而在标注过程中，后台会记录用户违反规范的次数，并适时给用户以警告，如果多次警告无效，则会冻结用户的标注权限。采用“3层检验” 机制，包括多用户标注一致性检验、与ASR结果一致性检验和专家审核检验3个层面。

参考文献：《大数据语音语料库的社会标注技术》
###Google's Neural Machine Translation System
论文：https://arxiv.org/pdf/1609.08144.pdf

神经机器翻译是一种端到端的机器翻译方法，并且有超越基于短语的机器翻译方法的潜力。不幸的是，在之前由于深度的循环神经网络在训练和预测阶段计算量都非常大，导致很难应用在庞大的数据集上，模型也不敢搞得太大，因此效果并不是很理想。这个机器翻译模型是一个有8个encoder和8个decoder组成的深度LSTM，为了加速在预测阶段的速度，在预测时采用了低精度运算。注意力机制加在ecoder的顶部和decoder的底部，并且encoder和decoder的不同阶段的运算使用了多GPU并行，提高了计算速度。
神经机器翻译的模型训练中，语料库就是训练集，这种端到端的训练方法简洁而优美，不需要加入人工的规则或者constrains。
### 利用Web构建大规模语料库的方法

最初语料库的构建大多是d小规模的，主要是因为人工参与过多，自动化程度不高，导致成本大、周期长，灵活性差，对语料库的更新困难，适应差，不能随着时间变化，自动适应当前的环境；不能实现个性化。大规模的语料库构建将会很大程度上加强语料库的作用。

半自动化的语料库构建模式是目前语料库建设的主流技术。对语料库构建方法d研究的主要目的是如何在确保语料库质量的前提下，减少人工参与的比例，增加自动化程度。利用网格构建一个信息处理平台，可以为语料库的建设提供计算和存储能力。生语料首先来自于Web网页，利用网格技术，可以很方便地使多台计算机同时下载网页，从而快速建设生语料库。该方法还可以结合自然语言处理技术对语料进行预处理，并利用网络的强大计算能力把生语料换成熟语料。

在该方法中，网格建立在Globus的sh基础上，提供了一批专门用于构建面向语料库建设的网格结点。这些节点主要提供以下功能：

1. 从Web站点获取文字信息，转换为文本后存放在生语料库中。主要由运行在网格结点的各个爬行器完成。

2. 处理生语料，转换为熟语料后放到语料库中。其核心是要根据具体的预料需要，用各种文字信息处理技术来处理并规范化预料。

3. 利用Wiki技术，提供添加预料以及对自动处理可信度不高的语料进一步进行人工甄别处理。

4. 提供语料的存储功能，把生语料和熟语料存放在数据库中，并利用数据库管理系统提供的分布技术实现语料的分布。

5. 提供语料发布功能，面向各类用户以服务形式提供语料服务。

利用网格的分布式和并行化的计算模式来收集f网站上的生语料，从而可加快收集语料的速度，为建设超大规模的语料库提供保障。

###语料库管理系统
经过科学选材和标注、具有适当规模的语料库，还应该有一个功能齐备的管理系统，包括数据维护（语料录入、校对、存储、修改、删除及语料描述信息项目管理）、语料自动加工（分词、标注、文本分割、合并、语料对齐、标记处理等）、用户服务功能（查询、检索、统计、打印等）。其中数据维护部分主要涉及汉字字符处理、文本处理、文件管理等计算机程序设计技术。语料自动加工部分的主要内容是自动分词、各种语言学属性的标注技术。

语料检索是一种全文检索技术，但是也有自己的特点，仅用普通的全文检索技术还不能满足语料检索的需要。这是因为，全文信息检索关心的是检索目标的意义，不是检索目标的语言表述形式。而面向语言研究的语料检索则特别注重语言的表述形式，它既需要按照字、字串和词检索，也需要把词语的语言学属性作为检索的目标和约束条件，还要求把检索的结果或目标的出处按照研究的需要排序、输出。除此之外，还要有字频、词频和特定语言形式出现频率的统计功能。

对汉语生语料的检索和统计是以字或字串为单位进行的。这一类检索系统主要以单字索引和字符串匹配为关键技术，由于把词语当作字串来检索，所以检索结果中经常出现“非词”的问题。要想从根本上解决这个问题，就必须对语料作词语切分。经过词语切分处理的熟语料，能以词为单位进行检索、统计和定量分析。但是熟语料库的加工代价很高，而且对于语料的词语切分和词性标注，目前还没有既成熟又便于操作的规范，所以近年来，面向生语料库的检索技术一直在广泛应用，并且在用户功能方面不断发展。

对于经过词语切分处理和词性标注的熟语料库，除了所有生语料的检索功能以外，语料检索系统还可以把词语或词性作为检索的关键字或限制条件，得到关于这些语言学属性的检索和统计结果，并按各种排序和输出形式的提供给用户。语言学属性来自语言学家对汉语的研究，研究过程中有各种观点和认识，从词的定义到词类的确定，一直还没有统一的意见。另一方面，人们检索语料时的目的也各不相同，有的关心词汇问题，有的关心语法现象，还有的目标是汉语信息处理的应用问题。因此对于熟语料库检索来说，一个好的检索系统应该能够包容各种不同的语言学观点，可以用于不同的检索目的。

为了做到这一点，通常采用的办法是，把用于语料库自动分词的底表和附着于底表的词性、构词等属性都看作语言学属性表，使这个属性表与检索系统的程序相互独立，检索系统只把属性标记作为抽象的字符串处理，而把建立属性表的工作交给用户。把语料加工技术集成在检索系统里面，是语料库检索系统的另一个特点。语料加工技术一般指词语自动切分和词性自动标注。

参考：语料库研究与应用综述
###汉语中介语语料库的收集和处理
新一代汉语中介语语料库的设计原则

1. 设计原则之一:词网技术

	网词网(网络词汇网)专业收集传统词典中没有的词汇,包括热门的最新的流行网络词汇、网络词语、网络语言、网络用语、网络流行语等。依频率形成反义、下位、部分词语义关系的词汇网络。需要用到的中文关键词分析工具,如“SEO 追词网冶淤,用于中文关键词的分析与挖掘。“SEO冶类似于非常实用的WordTracker 的关键词分析和挖掘功能。“WordTracker冶的关键词分析工具的功能包括五类:相关关键词、关键词趋势、关键词比较、网站趋势、关键词挖掘。“SEO 追词网冶具有竞争度分析、智能报价、关键词挖掘、友链检查、反链分析等五项功能。传统的人工描写方式存在搜索难度大和主观性强等困难。增设词网技术的语料库设计,可克服传统认知语义学存在的困难。
2. 设计原则之二:学习者特征

	在设计中介语语料库时,不仅考虑将收集到的文本内容汇集起来,还要加入学习者特征。对学习者的学习过程产生影响的因素,大致包括学习者的性别、年龄、个体认知能力、智能、学习动机、个人学习期望、社会背景、生活经历、经济和文化背景等因素,这些是学习者的一般特征。另外,学习者已具备母语能力、方言能力、以及其他外语能力等,学习者的学习风格等都需要考虑增设的特征要素。包含学习者特征的语料库能更真实的反映汉语中介语语料库的实用价值。
3. 设计原则之三:社会语言学特征

	汉语中介语语料库建设,既要重视语言交际的环境要素,也要重视学习者主体的个体差异和个体的认知系统。《中国大百科全书·语言文字》卷“社会语言学冶条目给社会语言学下的定义是:“研究语言的社会多方面关系的学科。它从不同的社会科学(诸如社会学、人类学、民族学、心理学、地理学、历史学等)的角度去考察语言,进而研究在不同社会条件下产生的语言变异。冶说明社会语言学重视研究应从不同学科视角研究语言变异的问题。它从七个方面归纳了布莱特(W.Bright)的社会语言学:
	(1)说话者的社会身份;(2)听话者的身份;(3)会话场景;(4)社会方言的历时研究、以及社会方言的共时研究;(5)平民语言学;(6)语言变异程度;(7)社会语言学的应用。20世纪90 年代后期,比利时学者Sylviane Granger 率先建立了英语学习者国际语料库(InternationalCorpus of Learner English,ICLE),开启了语料库与二语习得领域结合的研究。学习者语言是一个高变量,受多种多样的语言、情境和心理语言学因素的影响。Cobb 明确地指出:“认为语料库的建设只是从网络上收集大量的文本,然后将其组合起来的想法是一个普遍的错误想法。冶 Sylviane Granger 加入社会语言学特征的英语语料库设计,她认为学习者英语国际语料库(ICLE)的数据库包含了一系列影响二语特有的变量.

建设新一代汉语中介语语料库的信息管理系统：

1. 新一代互联网信息管理系统
2. 新一代网络信息抽取系统
3.  网络智能挖掘技术
4.  网络代理

建设汉语中介语语料库的目的是为全球汉语教学及相关研究服务,在使用中体现其价值。

基于大数据时代,建设国际一流水平的汉语中介语语料库,可概括为六个方面:

1. 在语料收集范围方面,能提供完整的语料;
2. 在语料内容方面,能提供跨学科研究的语料;
3. 在研究理论和方法方面,能应用跨学科的研究成果;
4. 在语料格式方面,能提供文字、语音、图像(如电影、动漫)等五官能接收的全息信息;
5. 在语料存取及处理技术方面,能融合各类语料存储、处理及使用技术(如云计算技术等);
6. 在语料库继续建设方面,能让使用者自行更新的智能分布式系统。

参考引用文献

郑通涛, 曾小燕. 大数据时代的汉语中介语语料库建设[J]. 厦门大学学报(哲学社会科学版), 2016(2):53-63.



### 语料库语言学研究的两种范式：渊源、分歧及前景
1、语料库语言学渊源之一： 基于语料库的研究范式
  1）1963年布朗语料库开始建立， 并于次年完成了这个容量为一百万词的第一个电子语料库。但当时语料库语言学在美国的发展并不好。
  2）而在英国，Quirk于1959年开始“英语用法调查”这一语料库建设项目。在很大程度上影响了语料库语言学的发展。继“ 英语用法调查” 之后，1990年代，“英语用法调查”项目组建成了国际英语语料库中的英国英语 部分，并对语料库进行了细致的句法标注。ICE-CUP的开发使得ICE的使用更为便捷，成为国际学界的常用语料库之一。
  3）之后，英语国家语料库的建设工作正式启动，并于1994年建成后对整个语料库进行了词性标注。
  4）布朗语料库、“ 英语用法调查” 及兰卡斯特大学所建设的大量语料库都是同根同源的。频繁的交流与合作增进了团队间的理解和共识， 逐渐形成了“基于语料库”的研究范式。

2、语料库语言学渊源之二： 语料库驱动的范式
  1）FIRTH在语言学领域主提出“意义”是语言学研究中的重点 而“意义”是由“语境”决定的。这一思想是他的“语境论”的基础；在弗斯看来，语言是一种社会行为，语言研究的主要目的就是分析意义。意义由语境决定“任何词使用在一个新的语境之中，就会成为一个新词”，因此研究意义需要从语境入手。 
  2）Halliday认为，“语料库驱动的语法并未做到不依赖任何理论”。
  3）这一学派以Sinclair的思想为理论基础，采用“语料库驱动”的研究范式，主张从语料库中建构理论、以全新视角对语言进行描写；他们关注搭配，同时也关注意义，认为搭配和意义是密不可分的。
  4）这一学派丰富了Firth关于搭配的理论和思想，建成了迄今为止世界上最大的语料库，并开创性地将语料库信息和辞书编纂结合起来，强化了词语为中心的语言学思想，并将搭配、类联接的研究与意义结合起来。

  
3、两种研究范式及其分歧
  1）不难看出，对已有语言分析体系应该抱何种态度，这是“ 基于语料库” 和“ 语料库驱动” 这两种研究范式间的根本差异。前者并不试图推翻已有的语言分析体系， 研究中只是将语料库视为众多数据种类中的一种，不排斥在必要时使用其他类型的数据（ 如诱发数据， 甚至内省数据），而后者主张一切源自语料库。
  2）关于语料库语言学学科属性的分歧：
    “ 语料库驱动”的研究范式认为，语料库语言学是一门独立的学科，摆脱任何已有的语言分类体系和研究框架，从 真实数据出发，对语言进行全新的描写。“基于语料库”的研究范式认为，语料库语言学并非一门独立的学科，而 是一种研究方法，可以用来验证已有假说和理论。
  3）关于研究目的和分析步骤的分歧
    “ 语料库驱动”的研究范式主要目的在于语言描写， 主张从观察词语入手， 反对利用语料库之外的任何理论前提。“ 基于语料库”的研究范式不排斥外部理论，目的在于验证已有假设。在这种研究范式中，词语不是研究的唯一切入点， 研究可以涉及性别、 语域等多种非语言因素。
  4）关于语料库标注的分歧
    “ 语料库驱动” 的研究范式主张“ 干净文本原则” 反对采用传统的语言分析体系对文本进行词性标注和句法标 注，而“基于语料库”的研究范式认为语料库的标注可以为语料库带来“ 增值”，方便从语料库中提取各种语言信息。
  5）关于语料库建设中文本取样方法的分歧
    话语分析是Sinclair关注的重要问题之一。在他看来，任何文本都有完整的内部结构，文本中的所有句子通过衔接和连贯等手段构成一个整体。Sinclair明确指出， 将文本切割成大小相同的片段，会破坏文本的整体性。相反，在“基于语料库”的研究范式中，为了尽可能保证语料库的代表性，语料库中文本的长短大致相等。
  6）关于语料库分析中统计学方法的差异
    由于“ 语料库驱动” 的研究范式主要目的在于语言描写，常常不需要对比，主要采用“ 词项－环境法”，分析技术主要依赖语料库语言学中的索引技术。相反，“ 基于语料库” 的研究范式沿用了普通实证研究的方法， 即提出假设、以数据验证假设。假设的验证常常需要对照组，比较和统计检验是研究中不可
缺少的环节。
  7）对非语料库语言学研究方法接受程度的分歧
    “ 语料库驱动” 的研究范式主张“ 相信文本”， 视语料库为语言研究的唯一数据源， 反对使用其他形式的数据。相反，“ 基于语料库” 的研究范式并不反对使用语料库之外其他形式的数据（ 如通过各种诱发手段获取的数据）。随着近年来语料库在各类研究中的广泛运用， 越来越多的研究者主张综合多种研究方法的结果， 进行“ 三角验证”。


参考引用文献

梁茂成. 语料库语言学研究的两种范式:渊源、分歧及前景[J]. 外语教学与研究, 2012(3):323-335.
